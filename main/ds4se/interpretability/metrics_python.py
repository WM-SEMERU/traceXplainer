# AUTOGENERATED! DO NOT EDIT! File to edit: dev/8.2_interpretability.metrics_python.ipynb (unless otherwise specified).

__all__ = ['compute_metrics', 'PythonAnalyzer']

# Cell

import pandas as pd
import pprint

# Cell

#Import of Metric 3rd party libraries
import radon
from radon.raw import analyze
from radon.complexity import *
from radon.metrics import *

# Cell

# TODO Add a Nan to columns where the registry cannot be computed instead of dropping it out
def compute_metrics(df_series):
    '''
    Computes metrics from source code

    Parameters:

    # df: Pandas dataframe containing source code column

    Returns:

    Tuple comprising:

    - Dataframe with computed metrics
    - List of records' indices for which metrics could not be computed

    '''

    init_data = {
        'sample': [],
        'loc':[],
        'lloc':[],
        'sloc': [],
        'comments':[],
        'multi': [],
        'blank':[],
        'single_comments':[],
        'h1': [],
        'h2': [],
        'N1': [],
        'N2': [],
        'vocabulary': [],
        'length': [],
        'calculated_length': [],
        'volume': [],
        'difficulty': [],
        'effort': [],
        'time': [],
        'bugs': [],
        'complexity': [],
        'maint_idx': [],
        'maint_idx_rank': []
    }

    # Empty DataFrame
    metrics_df = pd.DataFrame(init_data)

    problem_records_indices = []

    for idx, code in df_series.iteritems():
        new_row = {}
        try:
            # Computes available metrics if possible
            raw = analyze(code)
            halstead = h_visit(code)
            cc_met = cc_visit(code)
            maint_idx = mi_visit(code, False) # False indicates not to consider multi-line strings as comments
            maint_idx_rank = mi_rank(maint_idx)

            total_complexity = 0

            for func in cc_met:
                total_complexity += func.complexity

            new_row = {
                'sample': int(idx),
                'loc': raw.loc,
                'lloc': raw.lloc,
                'sloc': raw.sloc,
                'comments': raw.comments,
                'multi': raw.multi,
                'blank': raw.blank,
                'single_comments': raw.single_comments,
                'h1': halstead.total.h1,
                'h2': halstead.total.h2,
                'N1': halstead.total.N1,
                'N2': halstead.total.N2,
                'vocabulary': halstead.total.vocabulary,
                'length': halstead.total.length,
                'calculated_length': halstead.total.calculated_length,
                'volume': halstead.total.volume,
                'difficulty': halstead.total.difficulty,
                'effort': halstead.total.effort,
                'time': halstead.total.time,
                'bugs': halstead.total.bugs,
                'complexity': total_complexity,
                'maint_idx': maint_idx,
                'maint_idx_rank': maint_idx_rank
            }

        except:
            problem_records_indices.append(idx)
            new_row = {
            'sample': int(idx),
            'loc': float('nan'),
            'lloc': float('nan'),
            'sloc': float('nan'),
            'comments': float('nan'),
            'multi': float('nan'),
            'blank': float('nan'),
            'single_comments': float('nan'),
            'h1': float('nan'),
            'h2': float('nan'),
            'N1': float('nan'),
            'N2': float('nan'),
            'vocabulary': float('nan'),
            'length': float('nan'),
            'calculated_length': float('nan'),
            'volume': float('nan'),
            'difficulty': float('nan'),
            'effort': float('nan'),
            'time': float('nan'),
            'bugs': float('nan'),
            'complexity': float('nan'),
            'maint_idx': float('nan'),
            'maint_idx_rank': float('nan')
            }

        finally:
            metrics_df = metrics_df.append(new_row, ignore_index=True)

    if problem_records_indices:
        print(f'There was a problem computing metrics for {len(problem_records_indices)} records.')

    return metrics_df, problem_records_indices

# Cell

class PythonAnalyzer():
    """
    Class aimed to obtain metrics from a dataset of python source code records.
    Metrics computation is performed via an open source library.

    """

    def compute_metrics_for_df_series(self, df_series):
        """
        Computes metrics (static analysis) for a collection of source code records

        Params:
        # df_series: Pandas DF column containing source code records

        Returns:
        Tuple comprising

        - Pandas DataFrame with computed metrics
        - List of records' indices for which metrics could not be computed

        """

        return compute_metrics(df_series)


    def compute_and_save_metrics_for_df(self, df_series, destination_path):
        """
        Computes metrics (static analysis) for a pandas df column (series).
        Additionaly, exports metrics results to a csv file located at the specified path

        Params:
        # df_series: Pandas DF column containing source code records
        # destination_path: string indicating full path (including filename) for the exported file

        Returns:
        Tuple comprising

        - Pandas DataFrame with computed metrics
        - List of records' indices for which metrics could not be computed
        """

        metrics_df, error_indices = compute_metrics(df_series)

        metrics_df.to_csv(destination_path)

        return metrics_df, error_indices