# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/0.3_mgmnt.prep.bpe_tokenization.ipynb (unless otherwise specified).

__all__ = ['CustomTokenizer', 'HFTokenizer', 'SPTokenizer']

# Cell

import pandas as pd

from tokenizers import Tokenizer
import sentencepiece as spm

from abc import ABC, abstractmethod
from typing import List, Optional

from pathlib import Path

# Cell

# export
def _check_file_existence(path) -> bool:
    path = Path(path)
    if not path.exists():
        logging.error('Provided file cannot be found.')
        return False
    return True

# Cell

class CustomTokenizer(ABC):
    """
    Custom wrapper class to handle tokenizers.
    """

    def __init__(self, model_path: Optional[str]=None):
        if model_path is None:
            self.train_tokenizer()
        else:
            self.load_tokenizer_model(model_path)

    @abstractmethod
    def tokenize_txt(self, txt: str) -> List[str]:
        """
        Encode a text fragment according to the available
        tokens from the tokenizer model.

        :param txt: Text to be tokenized
        :return: List of str containing extracted tokens.
        """
        pass

    @abstractmethod
    def encode_txt(self, txt: str) -> List[int]:
        """
        Encode a text fragment according to the tokens ids.

        :param txt: Text to be encoded
        :return: List of int containing tokens ids.
        """
        pass

    def tokenize_df(self, df, txt_column) -> pd.DataFrame:
        """
        Perform tokenization
        :param df: DataFrame containing
        :param txt_column: Name of the column where to perform tokenization

        :return: DataFrame containing tokenized data
        """
        tokenized_df = df[txt_column].apply(lambda txt: self.tokenize_txt(txt))
        return tokenized_df

    @abstractmethod
    def load_tokenizer_model(self, model_path: str):
        """
        Load a pre-trained tokenizer model.
        :param model_path: Location of the pretrained model to be loaded
        """
        pass

    def train_tokenizer(self):
        """
        Train a tokenizer from scratch
        """
        # TODO

        msg = "Not implemented yet"
        logging.warning(msg)
        raise Exception(msg)

# Cell

class HFTokenizer(CustomTokenizer):
    """
    Custom wrapper of a HuggingFace tokenizer
    """

    def __init__(self, model_path: Optional[str]=None):
        super().__init__(model_path)

    def tokenize_txt(self, txt: str) -> List[str]:
        return self.tokenizer.encode(txt).tokens

    def encode_txt(self, txt: str) -> List[int]:
        return self.tokenizer.encoed(txt).ids

    def load_tokenizer_model(self, model_path: str):
        if not _check_file_existence(model_path):
            msg = "HF Tokenizer model could not be loaded."
            logging.error(msg)
            raise Exception(msg)

        self.tokenizer = Tokenizer.from_file(model_path)

# Cell

class SPTokenizer(CustomTokenizer):
    """
    Custom wrapper of SentencePiece tokenizer
    """

    def __init__(self, model_path: Optional[str]):
        super().__init__(model_path)

    def tokenize_txt(self, txt: str) -> List[str]:
        return self.tokenizer.encode_as_pieces(txt)

    def encode_txt(self, txt: str) -> List[int]:
        return self.tokenizer.encode_as_ids(txt)

    def load_tokenizer_model(self, model_path: str):
        if not _check_file_existence(model_path):
            msg = "SP model could not be loaded."
            logging.error(msg)
            raise Exception(msg)

        sp_processor = spm.SentencePieceProcessor()
        sp_processor.load(model_path)
        self.tokenizer = sp_processor