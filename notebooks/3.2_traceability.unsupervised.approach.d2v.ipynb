{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp traceability.unsupervised.approach.d2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting Neural Unsupervised Approaches for Software Information Retrieval [d2v]\n",
    "\n",
    "> This module is dedicated to evaluate doc2vec. Consider to Copy the entire notebook for a new and separeted empirical evaluation. \n",
    "> Implementing mutual information analysis\n",
    "> Author: @danaderp April 2020\n",
    "> Author: @danielrc Nov 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: File \"setup.py\" not found. Directory cannot be installed in editable mode: /tf/main/nbs\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install gensim\n",
    "#!pip install seaborn\n",
    "#!pip install sklearn\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This copy is for Cisco purposes. It was adapted to process private github data from cisco. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from itertools import product \n",
    "from random import sample \n",
    "import functools \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from datetime import datetime\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "from pandas.plotting import lag_plot\n",
    "import math as m\n",
    "import random as r\n",
    "import collections\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from gensim.models import WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html\n",
    "#export\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ds4se as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds4se.mgmnt.prep.conv import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experients Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../dvc-ds4se/' #dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_trained_model = path_data+'models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CISCO GitHub Parameters\n",
    "def sacp_params():\n",
    "    return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.issue2src,\n",
    "        \"system\": 'sacp-python-common',\n",
    "        \"path_to_trained_model\": path_data + 'models/wv/conv/[word2vec-Py-Java-Wiki-SK-500-20E[0]-1592979270.711115].model',\n",
    "        \"source_type\": SoftwareArtifacts.PR,\n",
    "        \"target_type\": SoftwareArtifacts.PY,\n",
    "        \"path_mappings\": '/tf/data/cisco/sacp_data/sacp-pr-mappings.csv',\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": '/tf/data/cisco/sacp_data/[sacp-python-common-all-corpus-1596383717.992744].csv', #MUST have bpe8k <----\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','conv'],\n",
    "            \"prep\": Preprocessing.conv\n",
    "        },\n",
    "        \"saving_path\":  path_data/'se-benchmarking/traceability/cisco/sacp',\n",
    "        \"names\": ['Source','Target','Linked?']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_trained_model = path_data + 'models/wv/bpe8k/[word2vec-Java-Py-Wiki-SK-500-20E-8k[12]-1594546477.788739].model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sacp_params_bpe():\n",
    "    return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.issue2src,\n",
    "        \"system\": 'sacp-python-common',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_type\": SoftwareArtifacts.PR,\n",
    "        \"target_type\": SoftwareArtifacts.PY,\n",
    "        \"path_mappings\": '/tf/data/cisco/sacp_data/sacp-pr-mappings.csv',\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": '/tf/data/cisco/sacp_data/[sacp-python-common-all-corpus-1596383717.992744].csv',\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','bpe8k'],\n",
    "            \"prep\": Preprocessing.bpe\n",
    "        },\n",
    "        \"saving_path\": path_data + 'se-benchmarking/traceability/cisco/sacp',\n",
    "        \"names\": ['Source','Target','Linked?'],\n",
    "        \"model_prefix\":path_data + 'models/bpe/sentencepiece/wiki_py_java_bpe_8k' #For BPE Analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = sacp_params_bpe()\n",
    "#parameters = sacp_params()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing experiments set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "parameters['system_path_config']['system_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "parameters['system_path_config']['names'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters['system_path_config']['sep'] #tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "df_all_system = pd.read_csv(\n",
    "            parameters['system_path_config']['system_path'], \n",
    "            #names = params['system_path_config']['names'], #include the names into the files!!!\n",
    "            header = 0, \n",
    "            index_col = 0, \n",
    "            sep = parameters['system_path_config']['sep'] \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_system.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "tag = parameters['system_path_config']['names'][1]\n",
    "[doc.split() for doc in df_all_system[df_all_system[tag].notnull()][tag].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all_system[tag].values) #tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "len(df_all_system[df_all_system[tag].notnull()]) #some files are _init_ thefore are empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "df_all_system[df_all_system[tag].notnull()][tag].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "df_all_system.loc[df_all_system['type'] == parameters['source_type']][parameters['system_path_config']['names']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_system.loc[df_all_system['type'] == parameters['target_type']][parameters['system_path_config']['names']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining BasicSequenceVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "print(list(VectorizationType), list(DistanceMetric), list(SimilarityMetric), list(LinkType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BasicSequenceVectorization():\n",
    "    '''Implementation of the class sequence-vanilla-vectorization other classes can inheritance this one'''\n",
    "    def __init__(self, params):\n",
    "                \n",
    "        self.params = params\n",
    "        self.df_nonground_link = None\n",
    "        self.df_ground_link = None\n",
    "        self.prep = ConventionalPreprocessing(params, bpe = True)\n",
    "        \n",
    "        self.df_all_system = pd.read_csv(\n",
    "            params['system_path_config']['system_path'], \n",
    "            #names = params['system_path_config']['names'], #include the names into the files!!!\n",
    "            header = 0, \n",
    "            index_col = 0, \n",
    "            sep = params['system_path_config']['sep'] \n",
    "        )\n",
    "        \n",
    "        #self.df_source = pd.read_csv(params['source_path'], names=['ids', 'text'], header=None, sep=' ')\n",
    "        #self.df_target = pd.read_csv(params['target_path'], names=['ids', 'text'], header=None, sep=' ')\n",
    "        self.df_source = self.df_all_system.loc[self.df_all_system['type'] == params['source_type']][params['system_path_config']['names']]\n",
    "        self.df_target = self.df_all_system.loc[self.df_all_system['type'] == params['target_type']][params['system_path_config']['names']]\n",
    "        \n",
    "        #NA verification\n",
    "        tag = parameters['system_path_config']['names'][1]\n",
    "        self.df_source[tag] = self.df_source[tag].fillna(\"\")\n",
    "        self.df_target[tag] = self.df_target[tag].fillna(\"\")\n",
    "        \n",
    "        if params['system_path_config']['prep'] == Preprocessing.conv: #if conventional preprocessing\n",
    "            self.documents = [doc.split() for doc in self.df_all_system[self.df_all_system[tag].notnull()][tag].values] #Preparing Corpus\n",
    "            self.dictionary = corpora.Dictionary( self.documents ) #Preparing Dictionary\n",
    "            logging.info(\"conventional preprocessing documents and dictionary\")\n",
    "        elif params['system_path_config']['prep'] == Preprocessing.bpe:\n",
    "            self.documents = [eval(doc) for doc in self.df_all_system[tag].values] #Preparing Corpus\n",
    "            self.dictionary = corpora.Dictionary( self.documents ) #Preparing Dictionary\n",
    "            logging.info(\"bpe preprocessing documents and dictionary\")\n",
    "            \n",
    "        ####INFO science params\n",
    "        abstracted_vocab = [ set(doc) for doc in self.df_all_system[ 'bpe8k' ].values] #creation of sets\n",
    "        abstracted_vocab = functools.reduce( lambda a,b : a.union(b), abstracted_vocab ) #union of sets\n",
    "        self.vocab = {self.prep.sp_bpe.id_to_piece(id): 0 for id in range(self.prep.sp_bpe.get_piece_size())}\n",
    "        dict_abs_vocab = { elem : 0 for elem in abstracted_vocab - set(self.vocab.keys()) } #Ignored vocab by BPE\n",
    "        self.vocab.update(dict_abs_vocab) #Updating\n",
    "        \n",
    "        \n",
    "        #This can be extended for future metrics <---------------------\n",
    "        #TODO include mutual and join information\n",
    "        self.dict_labels = {\n",
    "            DistanceMetric.COS:[DistanceMetric.COS, SimilarityMetric.COS_sim],\n",
    "            SimilarityMetric.Pearson:[SimilarityMetric.Pearson],\n",
    "            DistanceMetric.EUC:[DistanceMetric.EUC, SimilarityMetric.EUC_sim],\n",
    "            DistanceMetric.WMD:[DistanceMetric.WMD, SimilarityMetric.WMD_sim],\n",
    "            DistanceMetric.SCM:[DistanceMetric.SCM, SimilarityMetric.SCM_sim],\n",
    "            DistanceMetric.MAN:[DistanceMetric.MAN, SimilarityMetric.MAN_sim],\n",
    "            EntropyMetric.MSI_I:[EntropyMetric.MSI_I, EntropyMetric.MSI_X],\n",
    "            EntropyMetric.MI:[EntropyMetric.JI, EntropyMetric.MI]\n",
    "        }\n",
    "\n",
    "        \n",
    "    def ground_truth_processing(self, path_to_ground_truth = '', from_mappings = False):\n",
    "        'Optional class when corpus has ground truth. This function create tuples of links'\n",
    "        \n",
    "        if from_mappings:\n",
    "            df_mapping = pd.read_csv(self.params['path_mappings'], header = 0, sep = ',')\n",
    "            ground_links = list(zip(df_mapping['id_pr'].astype(str), df_mapping['doc_id']))\n",
    "        else:\n",
    "            ground_truth = open(path_to_ground_truth,'r')\n",
    "            #Organizing The Ground Truth under the given format\n",
    "            ground_links = [ [(line.strip().split()[0], elem) for elem in line.strip().split()[1:]] for line in ground_truth]\n",
    "            ground_links = functools.reduce(lambda a,b : a+b,ground_links) #reducing into one list\n",
    "            assert len(ground_links) ==  len(set(ground_links)) #To Verify Redundancies in the file\n",
    "        return ground_links\n",
    "    \n",
    "    def samplingLinks(self, sampling = False, samples = 10, basename = False):\n",
    "        \n",
    "        if basename:\n",
    "            source = [os.path.basename(elem) for elem in self.df_source['ids'].values ] \n",
    "            target = [os.path.basename(elem) for elem in self.df_target['ids'].values ]\n",
    "        else:\n",
    "            source = self.df_source['ids'].values\n",
    "            target = self.df_target['ids'].values\n",
    "\n",
    "        if sampling:\n",
    "            links = sample( list( product( source , target ) ), samples)\n",
    "        else:\n",
    "            links = list( product( source , target ))\n",
    "\n",
    "        return links\n",
    "    \n",
    "    def cos_scipy(self, vector_v, vector_w):\n",
    "        cos =  distance.cosine( vector_v, vector_w )\n",
    "        return [cos, 1.-cos]\n",
    "    \n",
    "    def euclidean_scipy(self, vector_v, vector_w):\n",
    "        dst = distance.euclidean(vector_v,vector_w)\n",
    "        return [dst, 1./(1.+dst)] #Computing the inverse for similarity\n",
    "    \n",
    "    def manhattan_scipy(self, vector_v, vector_w):\n",
    "        dst = distance.cityblock(vector_v,vector_w)\n",
    "        n = len(vector_v)\n",
    "        return [dst, 1./(1.+dst)] #Computing the inverse for similarity\n",
    "    \n",
    "    def pearson_abs_scipy(self, vector_v, vector_w):\n",
    "        '''We are not sure that pearson correlation works well on doc2vec inference vectors'''\n",
    "        #vector_v =  np.asarray(vector_v, dtype=np.float32)\n",
    "        #vector_w =  np.asarray(vector_w, dtype=np.float32)\n",
    "        logging.info(\"pearson_abs_scipy\" + str(vector_v) + \"__\" + str(vector_w))\n",
    "        corr, _ = pearsonr(vector_v, vector_w)\n",
    "        return [abs(corr)] #Absolute value of the correlation\n",
    "    \n",
    "\n",
    "    def computeDistanceMetric(self, links, metric_list):\n",
    "        '''Metric List Iteration''' \n",
    "        \n",
    "        metric_labels = [ self.dict_labels[metric] for metric in metric_list] #tracking of the labels\n",
    "        distSim = [[link[0], link[1], self.distance( metric_list, link )] for link in links] #Return the link with metrics\n",
    "        distSim = [[elem[0], elem[1]] + elem[2] for elem in distSim] #Return the link with metrics\n",
    "        \n",
    "        return distSim, functools.reduce(lambda a,b : a+b, metric_labels)\n",
    "    \n",
    "    def ComputeDistanceArtifacts(self, metric_list, sampling = False , samples = 10, basename = False):\n",
    "        '''Activates Distance and Similarity Computations\n",
    "        @metric_list if [] then Computes All metrics\n",
    "        @sampling is False by the default\n",
    "        @samples is the number of samples (or links) to be generated'''\n",
    "        links_ = self.samplingLinks( sampling, samples, basename )\n",
    "        \n",
    "        docs, metric_labels = self.computeDistanceMetric( metric_list=metric_list, links=links_) #checkpoints\n",
    "        self.df_nonground_link = pd.DataFrame(docs, columns =[self.params['names'][0], self.params['names'][1]]+ metric_labels) #Transforming into a Pandas\n",
    "        logging.info(\"Non-groundtruth links computed\")\n",
    "        pass \n",
    "    \n",
    "    \n",
    "    def SaveLinks(self, grtruth=False, sep=' ', mode='a'):\n",
    "        timestamp = datetime.timestamp(datetime.now())\n",
    "        path_to_link = self.params['saving_path'] + '['+ self.params['system'] + '-' + str(self.params['vectorizationType']) + '-' + str(self.params['linkType']) + '-' + str(grtruth) + '-{}].csv'.format(timestamp)\n",
    "        \n",
    "        if grtruth:\n",
    "            self.df_ground_link.to_csv(path_to_link, header=True, index=True, sep=sep, mode=mode)\n",
    "        else:\n",
    "            self.df_nonground_link.to_csv(path_to_link, header=True, index=True, sep=sep, mode=mode)\n",
    "        \n",
    "        logging.info('Saving in...' + path_to_link)\n",
    "        pass\n",
    "    \n",
    "    def findDistInDF(self, g_tuple, from_mappings=False, semeru_format=False):\n",
    "        '''Return the index values of the matched mappings\n",
    "        .eq is used for Source since it must match the exact code to avoid number substrings\n",
    "        for the target, the substring might works fine'''\n",
    "\n",
    "        if from_mappings:\n",
    "            dist = self.df_ground_link.loc[(self.df_ground_link[\"Source\"].eq(g_tuple[0]) ) & \n",
    "                 (self.df_ground_link[\"Target\"].str.contains(g_tuple[1], regex=False))]\n",
    "            logging.info('findDistInDF: from_mappings')\n",
    "        elif semeru_format:\n",
    "            dist = self.df_ground_link.loc[(self.df_ground_link[\"Source\"].str.contains(g_tuple[0], regex=False) ) & \n",
    "                 (self.df_ground_link[\"Target\"].str.contains(g_tuple[1], regex=False))]\n",
    "            logging.info('findDistInDF: semeru_format')\n",
    "        else:\n",
    "            dist = self.df_ground_link[self.df_ground_link[self.params['names'][0]].str.contains( g_tuple[0][:g_tuple[0].find('.')] + '-' ) \n",
    "                     & self.df_ground_link[self.params['names'][1]].str.contains(g_tuple[1][:g_tuple[1].find('.')]) ]\n",
    "            logging.info('findDistInDF: default')\n",
    "        return dist.index.values\n",
    "    \n",
    "        \n",
    "    def MatchWithGroundTruth(self, path_to_ground_truth='', from_mappings=False, semeru_format=False ):\n",
    "        self.df_ground_link = self.df_nonground_link.copy()\n",
    "        self.df_ground_link[self.params['names'][2]] = 0\n",
    "        \n",
    "        matchGT = [ self.findDistInDF( g , from_mappings=from_mappings, semeru_format=semeru_format ) for g in self.ground_truth_processing(path_to_ground_truth,from_mappings)]\n",
    "        matchGT = functools.reduce(lambda a,b : np.concatenate([a,b]), matchGT) #Concatenate indexes\n",
    "        new_column = pd.Series(np.full([len(matchGT)], 1 ), name=self.params['names'][2], index = matchGT)\n",
    "        \n",
    "        self.df_ground_link.update(new_column)\n",
    "        logging.info(\"Groundtruth links computed\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing BasicSequenceVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general2vec =  BasicSequenceVectorization(params = parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general2vec.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general2vec.documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general2vec.dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general2vec.df_all_system.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general2vec.df_all_system.shape #data final tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst for libest\n",
    "path_to_ground_truth = '/tf/main/benchmarking/traceability/testbeds/groundtruth/english/[libest-ground-req-to-tc].txt'\n",
    "general2vec.ground_truth_processing(path_to_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst for sacp\n",
    "general2vec.ground_truth_processing(parameters['path_mappings'], from_mappings = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import dit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to reproduce the same empirical evaluation like here: [link](https://arxiv.org/pdf/1507.07998.pdf). Pay attention to:\n",
    "- Accuracy vs. Dimensionality (we can replace accuracy for false positive rate or true positive rate)\n",
    "- Visualize paragraph vectors using t-sne\n",
    "- Computing Cosine Distance and Similarity. More about similarity [link](https://www.kdnuggets.com/2017/08/comparing-distance-measurements-python-scipy.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_trained_model\": 'test_data/models/pv/conv/[doc2vec-Py-Java-PVDBOW-500-20E-1592609630.689167].model',\n",
    "#\"path_to_trained_model\": 'test_data/models/pv/conv/[doc2vec-Py-Java-Wiki-PVDBOW-500-20E[15]-1592941134.367976].model',\n",
    "path_to_trained_model = 'test_data/models/[doc2vec-Py-Java-PVDBOW-500-20E-8k-1594572857.17191].model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec_params():\n",
    "    return {\n",
    "        \"vectorizationType\": VectorizationType.doc2vec,\n",
    "        \"linkType\": LinkType.req2tc,\n",
    "        \"system\": 'libest',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_path\": '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-req].csv',\n",
    "        \"target_path\": '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-tc].csv',\n",
    "        \"system_path\": '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-all].csv',\n",
    "        \"saving_path\": 'test_data/',\n",
    "        \"names\": ['Source','Target','Linked?']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_params = doc2vec_params()\n",
    "doc2vec_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "class Doc2VecSeqVect(BasicSequenceVectorization):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super().__init__(params)\n",
    "        self.new_model = gensim.models.Doc2Vec.load( params['path_to_trained_model'] )\n",
    "        self.new_model.init_sims(replace=True)  # Normalizes the vectors in the word2vec class.\n",
    "        self.df_inferred_src = None\n",
    "        self.df_inferred_trg = None\n",
    "        \n",
    "        self.dict_distance_dispatcher = {\n",
    "            DistanceMetric.COS: self.cos_scipy,\n",
    "            SimilarityMetric.Pearson: self.pearson_abs_scipy,\n",
    "            DistanceMetric.EUC: self.euclidean_scipy,\n",
    "            DistanceMetric.MAN: self.manhattan_scipy\n",
    "        }\n",
    "    \n",
    "    def distance(self, metric_list, link):\n",
    "        '''Iterate on the metrics'''\n",
    "        ν_inferredSource = list(self.df_inferred_src[self.df_inferred_src['ids'].str.contains(link[0])]['inf-doc2vec'])\n",
    "        w_inferredTarget = list(self.df_inferred_trg[self.df_inferred_trg['ids'].str.contains(link[1])]['inf-doc2vec'])\n",
    "        \n",
    "        dist = [ self.dict_distance_dispatcher[metric](ν_inferredSource,w_inferredTarget) for metric in metric_list]\n",
    "        logging.info(\"Computed distances or similarities \"+ str(link) + str(dist))    \n",
    "        return functools.reduce(lambda a,b : a+b, dist) #Always return a list\n",
    "    \n",
    "    def computeDistanceMetric(self, links, metric_list):\n",
    "        '''It is computed the cosine similarity'''\n",
    "        \n",
    "        metric_labels = [ self.dict_labels[metric] for metric in metric_list] #tracking of the labels\n",
    "        distSim = [[link[0], link[1], self.distance( metric_list, link )] for link in links] #Return the link with metrics\n",
    "        distSim = [[elem[0], elem[1]] + elem[2] for elem in distSim] #Return the link with metrics\n",
    "        \n",
    "        return distSim, functools.reduce(lambda a,b : a+b, metric_labels)\n",
    "\n",
    "    \n",
    "    def InferDoc2Vec(self, steps=200):\n",
    "        '''Activate Inference on Target and Source Corpus'''\n",
    "        self.df_inferred_src = self.df_source.copy()\n",
    "        self.df_inferred_trg = self.df_target.copy()\n",
    "        \n",
    "        self.df_inferred_src['inf-doc2vec'] =  [self.new_model.infer_vector(artifact.split(),steps=steps) for artifact in self.df_inferred_src['text'].values]\n",
    "        self.df_inferred_trg['inf-doc2vec'] =  [self.new_model.infer_vector(artifact.split(),steps=steps) for artifact in self.df_inferred_trg['text'].values]\n",
    "        \n",
    "        logging.info(\"Infer Doc2Vec on Source and Target Complete\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Doc2Vec SequenceVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec = Doc2VecSeqVect(params = doc2vec_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step1]Apply Doc2Vec Inference\n",
    "doc2vec.InferDoc2Vec(steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec.df_inferred_src.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_inferDoc2Vec_trg = inferDoc2Vec(df_target)\n",
    "#test_inferDoc2Vec_trg.head()\n",
    "doc2vec.df_inferred_trg.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(doc2vec.df_inferred_trg['inf-doc2vec'][0], doc2vec.df_inferred_trg['inf-doc2vec'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 2]NonGroundTruth Computation\n",
    "metric_l = [DistanceMetric.EUC,DistanceMetric.COS,DistanceMetric.MAN]# , SimilarityMetric.Pearson]\n",
    "doc2vec.ComputeDistanceArtifacts( sampling=False, samples = 50, metric_list = metric_l )\n",
    "doc2vec.df_nonground_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 3]Saving Non-GroundTruth Links\n",
    "doc2vec.SaveLinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_nonglinks_doc2vec = LoadLinks(timestamp=1594653325.258415, params=doc2vec_params)\n",
    "df_nonglinks_doc2vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 4]GroundTruthMatching Testing\n",
    "path_to_ground_truth = '/tf/main/benchmarking/traceability/testbeds/groundtruth/english/[libest-ground-req-to-tc].txt'\n",
    "doc2vec.MatchWithGroundTruth(path_to_ground_truth)\n",
    "doc2vec.df_ground_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 5]Saving GroundTruth Links\n",
    "doc2vec.SaveLinks(grtruth = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_glinks_doc2vec = LoadLinks(timestamp=1594653350.19946, params=doc2vec_params, grtruth = True)\n",
    "df_glinks_doc2vec.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach Evaluation and Interpretation (doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supervisedEvalDoc2vec = SupervisedVectorEvaluation(doc2vec, similarity=SimilarityMetric.EUC_sim)\n",
    "#supervisedEvalDoc2vec = SupervisedVectorEvaluation(doc2vec, similarity=SimilarityMetric.COS_sim)\n",
    "supervisedEvalDoc2vec = SupervisedVectorEvaluation(doc2vec, similarity=SimilarityMetric.MAN_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisedEvalDoc2vec.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisedEvalDoc2vec.y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisedEvalDoc2vec.Compute_precision_recall_gain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisedEvalDoc2vec.Compute_avg_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisedEvalDoc2vec.Compute_roc_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute distribution of similarities doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Statistics\n",
    "filter_doc2vec = doc2vec.df_ground_link\n",
    "filter_doc2vec.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot(filter_doc2vec[[SimilarityMetric.EUC_sim]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot(filter_doc2vec[DistanceMetric.EUC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_doc2vec.hist(column=[SimilarityMetric.EUC_sim,DistanceMetric.EUC],color='k',bins=50,figsize=[10,5],alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate distance from similarity analysis here\n",
    "errors = filter_doc2vec[[SimilarityMetric.EUC_sim,DistanceMetric.EUC]].std()\n",
    "print(errors)\n",
    "filter_doc2vec[[SimilarityMetric.EUC_sim,DistanceMetric.EUC]].plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_doc2vec.hist(by='Linked?',column=SimilarityMetric.EUC_sim,figsize=[10, 5],bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_doc2vec.hist(by='Linked?',column=DistanceMetric.EUC,figsize=[10, 5],bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate the distance from the similarity plot\n",
    "boxplot = filter_doc2vec.boxplot(by='Linked?',column=[SimilarityMetric.EUC_sim,DistanceMetric.EUC],figsize=[10, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = filter_doc2vec.boxplot(by='Linked?',column=[SimilarityMetric.EUC_sim],figsize=[10, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Doc2vec and Word2vec\n",
    "Please check this post for futher detatils [link](https://stats.stackexchange.com/questions/217614/intepreting-doc2vec-cosine-similarity-between-doc-vectors-and-word-vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_build_docs #<-------- [Activate when stable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
